{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing\n",
    "#### This notebook shows the process to extract the ppg, ecg, and blood pressure (abp) data from the .mat file, convert the data to time series and put them into a dataloader for training the LSTM. A training demo is also included to show that the dataloader works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !!!! If you want to run this notebook, change the file path\n",
    "file_path = \"G:/PycharmProjects/APS360Project/data/Part_1.mat\"\n",
    "# Open the .mat file using h5py.File()\n",
    "with h5py.File(file_path, \"r\") as hdf_file:\n",
    "    # List all the top-level keys (datasets, groups) in the HDF5 file\n",
    "    ref_key = list(hdf_file.keys())[0]\n",
    "    value_key = list(hdf_file.keys())[1]\n",
    "\n",
    "    data = hdf_file[value_key]\n",
    "    data = data[:200,0]\n",
    "\n",
    "    numpy_data = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        person = data[i]\n",
    "        obj = hdf_file[person]\n",
    "        obj = obj[:]\n",
    "        if obj.shape[0] > 5000:\n",
    "            obj = obj[1000:4000]    #take 3000 data points for each subject\n",
    "            numpy_data.append(obj)\n",
    "        if len(numpy_data) >84:    #take the first 84 subjects\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that the first 84 subjects' data are used. 3000 data points are selected from the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert numpy_data to numpy array\n",
    "numpy_data = np.array(numpy_data)\n",
    "#ppg_ecg is the 0th and 2nd channel\n",
    "ppg = numpy_data[:,:,0]\n",
    "ecg = numpy_data[:,:,2]\n",
    "ppg_ecg = np.stack((ppg, ecg), axis=2)\n",
    "abp = numpy_data[:,:,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I believe that in the paper, or at least in the repo that tried to replicate the paper's work, train test split are done on the subject level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_per = int(abp.shape[0] * 80 / 100)\n",
    "val_per = int(abp.shape[0] * 20 / 100)\n",
    "print('Num training set: ', train_per)\n",
    "print('Num testing set:  ', val_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_main, X_test, y_main, y_test = ppg_ecg[val_per:], ppg_ecg[:val_per], abp[val_per:], abp[:val_per]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "save the data to numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder_path = 'G:/PycharmProjects/APS360Project/data/'\n",
    "\n",
    "np.save(folder_path+'x_main.npy', X_main)\n",
    "np.save(folder_path+'y_main.npy', y_main)\n",
    "np.save(folder_path+'x_test.npy', X_test)\n",
    "np.save(folder_path+'y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T14:57:42.247297Z",
     "start_time": "2023-07-07T14:57:42.226836600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.load(folder_path+'x_main.npy',allow_pickle=True)\n",
    "y_train = np.load(folder_path+'y_main.npy',allow_pickle=True)\n",
    "X_test = np.load(folder_path+'x_test.npy',allow_pickle=True)\n",
    "y_test = np.load(folder_path+'y_test.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T14:57:42.261750600Z",
     "start_time": "2023-07-07T14:57:42.246299600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T14:57:42.270725500Z",
     "start_time": "2023-07-07T14:57:42.258877800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_to_2d(arr, index=None):\n",
    "    array = []\n",
    "    if index != None:\n",
    "        for subject_index in range(arr.shape[0]):\n",
    "            array.extend(arr[subject_index][:,index])\n",
    "    else:\n",
    "        for subject_index in range(arr.shape[0]):\n",
    "            array.extend(arr[subject_index])\n",
    "    return np.array(array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Basically concatenate all the data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T14:57:42.355637800Z",
     "start_time": "2023-07-07T14:57:42.271722700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ppg_train_conv = convert_to_2d(X_train, 0)\n",
    "ecg_train_conv = convert_to_2d(X_train, 1)\n",
    "abp_train_conv = convert_to_2d(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T14:58:19.175754600Z",
     "start_time": "2023-07-07T14:58:19.156194300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#max min scaleing without using sklearn\n",
    "ppg_train_conv_scaled = (ppg_train_conv - ppg_train_conv.min())/(ppg_train_conv.max()-ppg_train_conv.min())\n",
    "ecg_train_conv_scaled = (ecg_train_conv - ecg_train_conv.min())/(ecg_train_conv.max()-ecg_train_conv.min())\n",
    "abp_train_conv_scaled = (abp_train_conv - abp_train_conv.min())/(abp_train_conv.max()-abp_train_conv.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T14:57:42.363650100Z",
     "start_time": "2023-07-07T14:57:42.354638100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write a function to convert ppg_train_conv to time series, with batch size of 32\n",
    "def convert_to_timeseries(arr, batch_size):\n",
    "    array = []\n",
    "    i = 0\n",
    "    while i < arr.shape[0]-batch_size:\n",
    "        if (i+batch_size+2) % 3000 == 0:\n",
    "            i +=36\n",
    "            continue\n",
    "            #skip the next 34 rounds\n",
    "\n",
    "        array.append(arr[i:i+batch_size])\n",
    "        i+=1\n",
    "    return np.array(array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Convert the data to time series. Basically the first row is data points 1-31, the second row is data points 2-32, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T14:58:22.127614400Z",
     "start_time": "2023-07-07T14:58:21.212263400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert to time series\\\n",
    "ppg_train_timeseries_scaled = convert_to_timeseries(ppg_train_conv_scaled, 32)\n",
    "ecg_train_timeseries_scaled = convert_to_timeseries(ecg_train_conv_scaled, 32)\n",
    "abp_train_timeseries_scaled = convert_to_timeseries(abp_train_conv_scaled, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T21:04:47.426020200Z",
     "start_time": "2023-07-07T21:04:47.399239400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ppg_train_timeseries_scaled.shape)\n",
    "ppg_train_timeseries_scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "see how the second row is just moving first row to the left by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T14:59:07.111019400Z",
     "start_time": "2023-07-07T14:59:07.062975700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#put ppg and ecg together to form the encoder input with shape (201554,32,2)\n",
    "encoder_input = np.stack((ppg_train_timeseries_scaled, ecg_train_timeseries_scaled), axis=2)\n",
    "\n",
    "#stack the abp as well to form the decoder output with shape (201554,32,1)\n",
    "decoder_output = np.expand_dims(abp_train_timeseries_scaled, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T14:59:10.156940300Z",
     "start_time": "2023-07-07T14:59:10.141150500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T14:59:17.693628100Z",
     "start_time": "2023-07-07T14:59:17.649911700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataloader with batch size of 128\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_data = TensorDataset(torch.from_numpy(encoder_input).float(), torch.from_numpy(decoder_output).float())\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### The data preprocessing is finished.\n",
    "---------------------------------------------------------------------------\n",
    "Here is a training demo to show that the data loader is working."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Used Haroon's LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T14:59:14.805147Z",
     "start_time": "2023-07-07T14:59:12.195855300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class lstm_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(lstm_model, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=layer_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:26:32.515210Z",
     "start_time": "2023-07-07T20:26:31.958482500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_untrained = lstm_model(2, 64, 3, 1)   #not trained, for comparison\n",
    "model_trained = lstm_model(2, 64, 3, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Not sure if I did it the right way but I was trying to make the model relatively complex and see if it can overfit the data and predict the training data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:26:33.163045800Z",
     "start_time": "2023-07-07T20:26:33.146080900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train on gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_trained.to(device)\n",
    "model_untrained.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:33:03.537036500Z",
     "start_time": "2023-07-07T20:26:37.042107500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use dataloader to train the model\n",
    "import torch.optim as optim\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_trained.parameters(), lr=0.0003)\n",
    "#train the model\n",
    "num_epochs = 40\n",
    "loss_curve = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model_trained(inputs)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch: {}/{}, Step: {}/{}, Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "            loss_curve.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:34:11.023604300Z",
     "start_time": "2023-07-07T20:34:10.764929200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the loss curve\n",
    "plt.plot(loss_curve)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('training curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:36:24.172120500Z",
     "start_time": "2023-07-07T20:36:23.996992200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_trained = model_trained(torch.from_numpy(encoder_input[0:1280]).to(device).float())\n",
    "output_untrained = model_untrained(torch.from_numpy(encoder_input[0:1280]).to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T21:12:28.404006200Z",
     "start_time": "2023-07-07T21:12:28.355006200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_trained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:36:30.601348800Z",
     "start_time": "2023-07-07T20:36:30.353016300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compare the trained and untrained one to the ground truth\n",
    "plt.plot(output_trained.cpu().detach().numpy()[50,:,0], label='trained')\n",
    "plt.plot(output_untrained.cpu().detach().numpy()[50,:,0], label='untrained')\n",
    "plt.plot(decoder_output[50,:,0], label='ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It can be seen that in this batch, the training is working. Then we're converting the time series output back to 1d. The output structure is the same as the time series input, so like how we converted the input data to time series, we're converting the output time series back to 1d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:36:39.992787200Z",
     "start_time": "2023-07-07T20:36:39.980581700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert the time series output back to 1d\n",
    "def convert_to_1d(arr):\n",
    "    array = []\n",
    "    for i in range(arr.shape[0]):\n",
    "        if i == 0:\n",
    "            array.extend(arr[i,:,0])\n",
    "        else:\n",
    "            array.extend(arr[i,-1:,0])\n",
    "    return np.array(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:36:42.729070400Z",
     "start_time": "2023-07-07T20:36:42.700294200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_trained_1d = convert_to_1d(output_trained.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:36:44.193205400Z",
     "start_time": "2023-07-07T20:36:44.177493Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_trained_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:38:13.884379200Z",
     "start_time": "2023-07-07T20:38:13.547286900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot and compare to the grond truth\n",
    "plt.rcParams['figure.figsize'] = [25, 10]\n",
    "plt.plot(output_trained_1d[0:800], label='trained')\n",
    "plt.plot(abp_train_conv_scaled[0:800], label='ground truth')\n",
    "plt.ylabel('scaled abp')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It can be seen that the trained model is able to predict the training data well. Now we're going to use the trained model to predict the **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:44:25.930088500Z",
     "start_time": "2023-07-07T20:44:25.668385700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ppg_test_conv = convert_to_2d(X_test, 0)\n",
    "ecg_test_conv = convert_to_2d(X_test, 1)\n",
    "abp_test_conv = convert_to_2d(y_test)\n",
    "\n",
    "ppg_test_conv_scaled = (ppg_test_conv - ppg_test_conv.min())/(ppg_test_conv.max()-ppg_test_conv.min())\n",
    "ecg_test_conv_scaled = (ecg_test_conv - ecg_test_conv.min())/(ecg_test_conv.max()-ecg_test_conv.min())\n",
    "abp_test_conv_scaled = (abp_test_conv - abp_test_conv.min())/(abp_test_conv.max()-abp_test_conv.min())\n",
    "\n",
    "#convert to time series\n",
    "ppg_test_timeseries_scaled = convert_to_timeseries(ppg_test_conv_scaled, 32)\n",
    "ecg_test_timeseries_scaled = convert_to_timeseries(ecg_test_conv_scaled, 32)\n",
    "abp_test_timeseries_scaled = convert_to_timeseries(abp_test_conv_scaled, 32)\n",
    "\n",
    "#put ppg and ecg together to form the encoder input with shape (201554,32,2)\n",
    "encoder_input_test = np.stack((ppg_test_timeseries_scaled, ecg_test_timeseries_scaled), axis=2)\n",
    "#stacl abp as well to form the decoder output with shape (201554,32,1)\n",
    "decoder_output_test = np.expand_dims(abp_test_timeseries_scaled, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:49:04.967996900Z",
     "start_time": "2023-07-07T20:49:04.950047100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predict the abp with the encoder input test\n",
    "output_test = model_trained(torch.from_numpy(encoder_input_test[0:1280]).to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:50:38.784090100Z",
     "start_time": "2023-07-07T20:50:38.735517600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert the output to 1d\n",
    "output_test_1d = convert_to_1d(output_test.cpu().detach().numpy())\n",
    "output_test_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T20:50:59.905760800Z",
     "start_time": "2023-07-07T20:50:59.386138600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the predicted abp and the ground truth\n",
    "plt.rcParams['figure.figsize'] = [25, 10]\n",
    "plt.plot(output_test_1d[0:800], label='trained')\n",
    "plt.plot(abp_test_conv_scaled[0:800], label='ground truth')\n",
    "plt.ylabel('scaled abp')\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Unfortunately the test data prediction is bad. That make sense though, since this is just a training demo. Maybe the model is overfitted, we gotta put more work into the training process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
